{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpK7hS7dHwuCd94tJ5Uph4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yschimpf/feedback_alignment/blob/main/backpropagtion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RK5tWzJlRqGl"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(object):\n",
        "  #layers include input and output layer, activiation allows for an activation\n",
        "  #to allow for f.e. softmax in case of application\n",
        "  def __init__(self, layers = [2,10,1], activations = ['sigmoid','identity'], loss = 'MSE'):\n",
        "    assert(len(layers) == len(activations)+1)\n",
        "    self.layers = layers\n",
        "    self.activations = activations\n",
        "    self.loss = loss\n",
        "    self.weights = []\n",
        "    self.biases = []\n",
        "    #fill the matrices with random values\n",
        "    for i in range (len(layers)-1):\n",
        "      self.weights.append(np.random.randn(layers[i+1], layers[i]))\n",
        "      self.biases.append(np.random.randn(layers[i+1], layers[i]))\n",
        "\n",
        "    # computation of all unit inputs and activations fpr all layers\n",
        "    def feedfoward(self, x):\n",
        "      #first layer has no activation function\n",
        "      v = np.copy(x)\n",
        "      z_s = []\n",
        "      v_s = [v]\n",
        "      #for all other leayers compute the weighted inputs of last layer units\n",
        "      #and apply activations. Last layer has activation to f.e. apply softmax\n",
        "      for i in range(len(self.weights)):\n",
        "        activation = self.getActivationFunction(self.activation[i])\n",
        "        z_s.append(self.weights[i].dot(v)+self.biases[i])\n",
        "        v = activation(z_s[-1])\n",
        "        v_s.append(v)\n",
        "      return (z_s, v_s)\n",
        "\n",
        "\n",
        "    #gets labels and feedforward data\n",
        "    #returns derivatives w.r.t. weight matrices\n",
        "    def backpropagation(self, y, z_s, v_s):\n",
        "      #derivatives with respect to the weighs\n",
        "      dw = []\n",
        "      #error backprogated through the layers i.e. dL/dz i.e. error\n",
        "      #in terms of inputs to the respective layer\n",
        "      deltas = [None]*len(self.weights)\n",
        "      #error in terms of inputs of last layer\n",
        "      deltas[-1] = (self.getDerivativeLoss(self.loss)(v_s[-1], y)) * (self.getDerivativeActivation(self.activations[-1])(z_s[-1]))\n",
        "      #actual backpropagation\n",
        "      for i in reversed(range(len(deltas)-1)):\n",
        "        #compute dL/dz for each layer that is the loss in terms of the inputs of the activation\n",
        "        deltas[i] = self.weights[i+1].T.dot(deltas[i+1])*(self.getDerivativeActivation(self.activations[i])(z_s[i]))\n",
        "      batch_size = y.shape[1]\n",
        "      #compute dL/dw for all weight matrices\n",
        "      dw = [d.dot(v_s[i].T)/float(batch_size) for i,d in enumerate(deltas)]\n",
        "      #compute dL/db for all biases\n",
        "      db = [d.dot(np.ones((batch_size,1)))/float(batch_size) for i,d in enumerate(deltas)]\n",
        "      return dw, db\n",
        "\n",
        "    #train netowrk on the training data\n",
        "    def train(self, x, y, batch_size=10, epochs=100, lr = 0.01):\n",
        "      for e in range (epochs):\n",
        "        i = 0\n",
        "        while(i < len(y)):\n",
        "          x_batch = x[i:i+batch_size]\n",
        "          y_batch = y[i:i+batch_size]\n",
        "          i = i+batch_size\n",
        "          z_s, v_s = self.feedforward(x_batch)\n",
        "          dw, db = self.backpropagation(y_batch, z_s, v_s)\n",
        "          self.weights = [w - lr*dweight for w,dweight in zip(self.weights, dw)]\n",
        "          self.biases = [b - lr*dbias for b,dbias in zip(self.biases, db)]\n",
        "          print(\"loss ={}\".format(np.linalg.norm(v_s[-1]-y_batch)))\n",
        "\n",
        "    #definition of nonlinearities for units\n",
        "    @staticmethod\n",
        "    def getActivationFunction(name):\n",
        "      if(name == 'sigmoid'):\n",
        "        return lambda x : np.exp(x)/(1+np.exp(x))\n",
        "      elif(name == 'identity'):\n",
        "        return lambda x : x\n",
        "      elif(name == 'relu'):\n",
        "        def relu(x):\n",
        "          y = np.copy(x)\n",
        "          if(y<0):\n",
        "            return 0\n",
        "          else:\n",
        "            return y\n",
        "        return relu\n",
        "      else:\n",
        "        print('Unknown activation function. Identity is used')\n",
        "        return lambda x:x\n",
        "\n",
        "    #derivative of activation\n",
        "    @staticmethod\n",
        "    def getDerivativeActivation(name):\n",
        "      if(name == 'sigmoid'):\n",
        "        sigmoid = lambda x : np.exp(x)/(1+np.exp(x))\n",
        "        return lambda x : sigmoid(x)*(1-sigmoid(x))\n",
        "      elif(name == 'identity'):\n",
        "        return lambda x : 1\n",
        "      elif(name == 'relu'):\n",
        "        def drelu(x):\n",
        "          y = np.copy(x)\n",
        "          if(y >= 0): y = 1\n",
        "          else: y = 0\n",
        "          return y\n",
        "        return drelu\n",
        "      else:\n",
        "        print('Unknown activation, identity was used instead')\n",
        "        return lambda x:1\n",
        "\n",
        "\n",
        "    #derivative of loss\n",
        "    @staticmethod\n",
        "    def getDerivativeLoss(name):\n",
        "      #compute derivative of MSE for formula L = 1/2 sum_{i}(f_i - y_i)^2 w.r.t. f_i\n",
        "      #where i sums over the samples, f_i is the network output and y-i the label\n",
        "      if(name == 'MSE'):\n",
        "        return lambda f, y : (f-y)\n",
        "      #without a specified loss computing a loss does not really make sense\n",
        "      else:\n",
        "        print('Unknown loss, cannot make useful computations')\n",
        "        exit()\n",
        ""
      ],
      "metadata": {
        "id": "LIWHDAwsSVrN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YtUpoAqeTcwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing stuff\n",
        "arr = np.array([1,2,3,4])\n",
        "print(arr[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_FabggYH9k1",
        "outputId": "c17f161d-4fea-4ed9-c25c-b9a44f523940"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    }
  ]
}
